---
title: "p8106_hw1_yt2785"
author: "Yijing Tao yt2785"
output:
  pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ISLR)
library(glmnet)
library(caret)
library(corrplot)
library(plotmo)
library(FNN) # knn.reg()
library(doBy) # which.minn()
library(pls)
```

```{r}
train_df = read_csv("./housing_training.csv") %>% 
  data.frame() %>% 
  na.omit()
train_matrix <- model.matrix(Sale_Price~ ., train_df)[ ,-1] 

test_df = read_csv("./housing_test.csv")
  data.frame() %>% 
  na.omit()
test_matrix <- model.matrix(Sale_Price~ ., test_df)[ ,-1] 
  
# matrix of predictors (glmnet uses input matrix)
x <- train_matrix
# vector of response
y <- train_df$Sale_Price

ctrl1 <- trainControl(method = "cv", selectionFunction = "oneSE")
ctrl2 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
```

## a) Fit a linear model using least squares on the training data. Is there any potential disadvantage of this model?

```{r}
lm.fit <- lm(Sale_Price ~ ., data = train_df)
summary(lm.fit)
lm.pred <- predict(lm.fit, newdata = test_df)
# test error
test_error_lm = mean((lm.pred - test_df$Sale_Price)^2)
test_error_lm
```
**The test error of the LS linear model is `r test_error_lm`. \**
**The potential disadvantage of this model might be the correlation between different predictors.Although the adjusted R square is high in this model, there might be over fitting in this model. And the p-value of this model is less than 0.05, which means this model is not reliable.**

## b) Fit a lasso model on the training data and report the test error. When the 1SE rule is applied, how many predictors are included in the model?

```{r}
# check the range of lambda
set.seed(1)
cv.lasso <- cv.glmnet(x, y, 
                      alpha = 1, 
                      lambda = exp(seq(8, 3, length = 100)))
plot(cv.lasso)

# fit the lasso model
set.seed(1)
lasso.fit.minse <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(8, 3, length=100))),
                   trControl = ctrl2)
lasso.fit.minse$bestTune
plot(lasso.fit.minse, xTrans = log)
lasso.pred <- predict(lasso.fit.minse, newdata = test_matrix)
# test error
test_error_lasso_minse = mean((lasso.pred - test_df$Sale_Price)^2)
test_error_lasso_minse

# apply the 1SE rule
set.seed(1)
lasso.fit.1se <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(8, 3, length=100))),
                   trControl = ctrl1)

summary(lasso.fit.1se)
precictor_num = sum((coef(lasso.fit.1se$finalModel, lasso.fit.1se$bestTune$lambda)) != 0)
precictor_num
lasso.pred <- predict(lasso.fit.1se, newdata = test_matrix)
# test error
test_error_lasso_1se = mean((lasso.pred - test_df$Sale_Price)^2)
test_error_lasso_1se
```
**From the first plot we can decide that lambda should be set between exp(8) to exp(3), `r precictor_num` predictors are included in the model when 1SE rule is applied. The test error of the minse lasso model is `r test_error_lasso_minse` and the The test error of the 1se lasso model is `r test_error_lasso_1se`**


## c) Fit an elastic net model on the training data. Report the selected tuning parameters and the test error.

```{r}
set.seed(1)
enet.fit <- train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 20), 
                                         lambda = exp(seq(8, 4, length = 60))),
                  trControl = ctrl2)
enet.alpha.best = enet.fit$bestTune$alpha
enet.alpha.best
enet.lambda.best = enet.fit$bestTune$lambda
enet.lambda.best
plot(enet.fit)


enet.pred <- predict(enet.fit, newdata = test_matrix)
# test error
test_error_enet = mean((enet.pred - test_df$Sale_Price)^2)
test_error_enet
```
**The selected best parameter is alpha = `r enet.alpha.best` and lambda = `r enet.lambda.best`. The test error is `r test_error_enet`.**

## d) Fit a partial least squares model on the training data and report the test error. How many components are included in your model?

```{r}
set.seed(1)
pls.mod <- plsr(Sale_Price ~ ., 
                data = train_df, 
                scale = TRUE,  
                validation = "CV")

summary(pls.mod)
validationplot(pls.mod, val.type="MSEP", legendpos = "topright")

cv.mse <- RMSEP(pls.mod)
ncomp.cv <- which.min(cv.mse$val[1,,])-1
ncomp.cv

predy2.pls <- predict(pls.mod, newdata = test_df, 
                      ncomp = ncomp.cv)
# test MSE
test_error_pls = mean((predy2.pls - test_df$Sale_Price)^2)
test_error_pls
```
**8 components are included in the model. The test error of this model is `r test_error_pls`.**

## e) Which model will you choose for predicting the response? Why?

```{r}
mse <- c(test_error_lasso_minse, test_error_lasso_1se, test_error_enet, test_error_pls)
name <- c("Lasso(minse)", "Lasso(1se)", "Elastic", "PLS")
MSE_df <- cbind(name, mse)
colnames(MSE_df) <- c("Model", "MSE")
MSE_df <- as.data.frame(MSE_df)
MSE_df
which.min(MSE_df$MSE)
```
**From question a) we can consider that the LS linear model is not reliable, so I decided not to include it in the comparision. I will choose the lasso model which applied 1SE since it's MSE is the smallest among these models, which means it has the highest accuracy and efficiency.**
